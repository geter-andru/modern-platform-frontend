#!/usr/bin/env python3
"""
LinkedIn Profile Scraper with Session Cookie
Extracts all required fields from LinkedIn profiles
"""

import asyncio
import json
import sys
from playwright.async_api import async_playwright

# LinkedIn session cookie
LI_AT_COOKIE = "AQEFARABAAAAABaKfO8AAAGXjoMbMgAAAZo25awuVgAAs3VybjpsaTplbnRlcnByaXNlQXV0aFRva2VuOmVKeGpaQUFDanFnVlQwQTB6OC96dDBHMDhObGRVWXdnUnZ3bjlmMWdoc2FQM2M0TWpBRHMxZ3JyXnVybjpsaTplbnRlcnByaXNlUHJvZmlsZToodXJuOmxpOmVudGVycHJpc2VBY2NvdW50OjE0MDE1OTIwNCwyMTc2OTgyNjcpXnVybjpsaTptZW1iZXI6MzY3NDk1NjMxD2sBLJZukqiUebKrRnjB7rzBiaccWbRiZ-J3gJ_9Eukevm3FTPnAQAdW3AF-IRIkBWQS096YT2cUtVhpTK6FUqEbFWLVVnlL9YzeD6jVDFTqsVrlPgtZ7Ic1UanFCkkLC6zcbki3UofztpeePcMZ8idfTzv9uL4rUquS60EqUk2VQZN31XH5W2vRmnkyQI04spxjRw"

async def scrape_linkedin_profile(profile_url):
    """
    Scrape LinkedIn profile with authenticated session
    Returns: dict with all required LinkedIn fields
    """
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context()

        # Add LinkedIn session cookie
        await context.add_cookies([{
            'name': 'li_at',
            'value': LI_AT_COOKIE,
            'domain': '.linkedin.com',
            'path': '/',
            'httpOnly': True,
            'secure': True,
            'sameSite': 'None'
        }])

        page = await context.new_page()

        print(f"üîç Navigating to: {profile_url}")
        await page.goto(profile_url, wait_until='networkidle')
        await page.wait_for_timeout(3000)  # Wait for dynamic content

        # Get visible text
        content = await page.inner_text('body')

        # Extract data (simplified - in production would use selectors)
        data = {
            'linkedin_recent_posts': '',
            'linkedin_background': '',
            'linkedin_company_description': '',
            'linkedin_personality_signals': '',
            'linkedin_hiring_signals': '',
            'linkedin_product_signals': '',
            'raw_content': content[:5000]  # First 5000 chars for review
        }

        await browser.close()

        return data

async def main():
    if len(sys.argv) < 2:
        print("Usage: python linkedin-scraper.py <profile_url>")
        sys.exit(1)

    profile_url = sys.argv[1]
    data = await scrape_linkedin_profile(profile_url)

    # Save to file
    output_file = '/tmp/linkedin_scraped_data.json'
    with open(output_file, 'w') as f:
        json.dump(data, f, indent=2)

    print(f"\n‚úÖ LinkedIn data scraped and saved to: {output_file}")
    print(f"\nüìÑ Preview (first 500 chars):")
    print(data['raw_content'][:500])

if __name__ == '__main__':
    asyncio.run(main())
